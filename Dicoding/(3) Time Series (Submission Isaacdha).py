# -*- coding: utf-8 -*-
"""Time Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fjeZbhPs66qHkFY77SGhHA-JOvtAEqzJ
"""

# Commented out IPython magic to ensure Python compatibility.
#Dependecies
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from sklearn.model_selection import train_test_split
from keras.layers import Bidirectional, Dropout, Activation, Dense, LSTM, CuDNNLSTM

import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rc
from pylab import rcParams
from sklearn.preprocessing import MinMaxScaler

#setting the plot
# %matplotlib inline
rcParams['figure.figsize'] = 14, 8
sns.set(style='whitegrid', palette='muted', font_scale=1.5)

#Input Data
Data = pd.read_csv('/content/drive/My Drive/Dicoding/Submission/Submission_2/Coin_Ethereum.csv', parse_dates=['Date'])
Data

"""Sumber Data : https://www.kaggle.com/sudalairajkumar/cryptocurrencypricehistory?select=coin_Ethereum.csv"""

#Drop Baris Kosong
Data.dropna(inplace=True)

#Mengambil data yang diperlukan
Harga = Data['Close'].values
Tanggal = Data['Date'].values

#Bentuk Plot Time Series
plt.plot(Tanggal, Harga)
plt.xlabel('Tanggal')
plt.ylabel('Harga (USD)')

#Mengonvert Data menjadi Data yang diaccept Model
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

#Membuat Model Sequensial
Model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=32, kernel_size=5,strides=1,padding='causal',activation='relu',input_shape=[None,1]),
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(64),
  tf.keras.layers.Dense(128, activation="relu"),
  tf.keras.layers.Dense(64, activation="relu"),
  tf.keras.layers.Dense(32, activation="relu"),
  tf.keras.layers.Dense(16, activation="relu"),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x*400)
])

#Split Dataset Yang Telah Dibuat
x_train, x_test, y_train, y_test = train_test_split(Harga, Tanggal, test_size = 0.2, random_state = 0, shuffle = False)

#Memwindowing Dataset
train_set = windowed_dataset(x_train, window_size=60, batch_size=120, shuffle_buffer=1200)
test_set = windowed_dataset(x_test, window_size=60, batch_size=120, shuffle_buffer=1200)

#Optimizer
Optimizer = tf.keras.optimizers.Adam(learning_rate=1e-8)

#Learning Rate dari Lamda
lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))

#Compile Model
Model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=["mae"])

#10% Syarat Dicoding MAE
m = 0.1*(Harga.max()-Harga.min())

print('Batas MAE =',m, 'Dollar')

#Train Model
history = Model.fit(train_set ,epochs=30, validation_data= test_set)

#Plot Model Train
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('MAE')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

#Plot Model Test
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

"""Model telah baik dan tidak melebihi batas 10% MAE, namun model terlihat overfitting sehingga saat digunakan di test set MAE lebih dari batas dan tidak baik."""