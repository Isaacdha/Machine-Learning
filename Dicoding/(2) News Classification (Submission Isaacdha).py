# -*- coding: utf-8 -*-
"""News_Classification_Submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v1SD_9zkUU5j2-5v24hiYlg19MqmgZGm
"""

# Dependencies
!pip install nltk
import pandas as pd
import re
import matplotlib.pyplot as plt
import tensorflow as tf

from sklearn.model_selection import train_test_split

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout

Data = pd.read_csv('/content/drive/My Drive/Dicoding/Submission/Submission_1/Data_Berita.csv', sep=';')
Data.head()

"""Sumber Data https://www.kaggle.com/akash14/news-category-dataset"""

# Preprocessing
# Menghilangkan Baris yang memiliki data kosong
Data.dropna(inplace=True)

# Menghapus Tanda Baca pada Story
Data['Story'] = Data['Story'].map(lambda x: re.sub(r'\W+',' ', x))

# Menghapus Stopwords
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop = set(stopwords.words('english'))
Data['Story'] = Data['Story'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

Data

#count values for each category
plt.figure(figsize=(10,8))
Data.groupby('Category').count().plot.bar(ylim=0)
plt.show()

"""Jumlah Setiap Kelas hampir sama, Sehingga tidak diperlukan perlakuan lebih lanjut

"""

category = pd.get_dummies(Data.Category)
Data = pd.concat([Data, category], axis=1)
Data = Data.drop(columns='Category')
Data

#Convert Data string ke np.array
text = Data['Story'].astype(str)
label = Data[['Business','Entertainment','Politics','Technology']].values

#Split Data 80,20
story_train, story_test, label_train, label_test = train_test_split(text, label,test_size = 0.2)

#Tokenizing
token = Tokenizer(num_words= 10000, oov_token = 'x')

token.fit_on_texts(story_train)
token.fit_on_texts(story_test)
tok_train = token.texts_to_sequences(story_train)
tok_test= token.texts_to_sequences(story_test)

#Padding
pad_train = pad_sequences(tok_train)
pad_test = pad_sequences(tok_test)

"""Membuat Model"""

#Membuat Model Sequential
Model = tf.keras.Sequential([
    Embedding(input_dim=10000, output_dim=16),
    LSTM(64),
    Dense(128, activation='elu'),
    Dense(256, activation='elu'),
    Dense(128, activation='elu'),
    Dense(64, activation='selu'),
    Dropout(0.5),
    Dense(4, activation='softmax')
])

#Setting Optimizer Adam
Adam(learning_rate = 0.001, name = 'Adam')

#Compile Model
Model.compile(loss='categorical_crossentropy',
              optimizer='Adam',
              metrics=['accuracy'])

#Set Callback
class Panggilkembali(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>0.95):
      print("\nAkurasi validasi sudah > 95%!")
      self.model.stop_training = True
callbacks = Panggilkembali()

#Train Model
history = Model.fit(pad_train, label_train, epochs=100, validation_data=(pad_test, label_test), verbose=3, callbacks=[callbacks])

#acc plot
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()

#loss plot
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

Model.evaluate(pad_test, label_test)

"""Akurasi Model pada Test (Val) Dataset telah 95%"""

Model.evaluate(pad_train, label_train)

"""Akurasi Model pada Train Dataset telah 99,7%"""